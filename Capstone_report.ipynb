{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Allstate Claim Severity Capstone Project\n",
    "Jose Garcia  \n",
    "December 31st, 2050\n",
    "\n",
    "## I. Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview\n",
    "\n",
    "Allstate is on the largest personal insurance companies in the United states and protects millions of people from potentials damages. When car accidents happen or when other unexpected disasters occur it is a huge relief knowing you are cover by some type of insurance. Recently my girlfriend was involved in a car accident that almost totaled her car. Although she escaped with no bodily injuries her car had thousands of dollars in damages. Luckily, she has full coverage insurance and was able to get all repairs paid for and a rental car without paying anything out of pocket. This event peaked my interest in to insurance business which lead me to this project. Although for this particular instance the insurance claim process was easy and straight forward it is not always the case. This is why Allstate is currently developing automated methods of predicting the cost, and hence severity, of a claim. This project will provide insight into better ways to predict claims severity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets and Inputs\n",
    "\n",
    "For this project Allstate provides a test.csv and train.csv. \n",
    "\n",
    "The train.csv contains the following:\n",
    " * claim id\n",
    " * cat 1 to cat 116 - this data is category based and contains either single letters or two letters\n",
    " * cont1 to cont14 - this data is a continues set of numbers that are not negative\n",
    " * loss - this is the ammount that the insurance company has to payout and this is also the target variable\n",
    " * 188,318 rows\n",
    " * 132 columns\n",
    " \n",
    "The test.csv file containt the same information as train.csv except for the loss because it needs to be predicted.\n",
    " * 89,023 rows\n",
    " * 131 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "Allstate is attempting to improve its customer experience by automating methods of predicting the severity of a claim. The severity of a claim is based on a combination of different parameters that are unique to every claim. An insurance adjuster takes a look at each claim and all of its parameters and determines how much will be paid out tot he customer. In some cases the claim is quickly resolved but there is some instances in which it takes a long time to resolve the claim. For this project a training set of data will be used to train a machine learning algorithm to predict the cost of a claim based on the given parameters of the claim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Kaggle will be evaluating this project using mean absolute error and the objective is to get the lowest error possible. Of course, there are many other ways to evaluate this model but for the Kaggle competition this is the way the project entries are evaluated. Mean absolute error is a measure of the average difference between the actual loss and predicted loss.\n",
    "\n",
    "<img src=\"mean-absolute-error.png\" width=\"300\">\n",
    "### <center>Figure 1: Mean Aboslute Erro</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the first five rows of the \"trian.csv\" data set. The data set is composed of an id number, category data, continues data, and a column labeled \"loss\". There is a total of 188,318 rows and 132 columns of data but not all of these parameters are useful for building our predictive model. The column labeled \"id\" will be dropped because it does not add any value to our algorithm. The \"loss\" column will also be dropped because this is the value that needs to be predicted and should not be mixed in with our features. Now there are only two types of data, non-numeric, which is composed of letters and numerical data which is composed of continues numbers. There is a total of 14 columns of continues data and 116 columns of non-numeric data. Right below the \"train.csv\" data set is the \"test.csv\" data which is exactly the same except that it does not have the \"loss\" column. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"train_data.png\" width=\"1000\">\n",
    "### <center>Figure 2: train.csv</center>\n",
    "\n",
    "<img src=\"test_data.png\" width=\"1000\">\n",
    "### <center>Figure 3: test.csv</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "From this point on the \"train.csv\" data is the only data set that will be analyzed.\n",
    "\n",
    "In order to get a better understanding of the data it must be represented in different forms in order to get different types of information. Running basic statistical analysis on the continues data reveals key facts about the data. A few things to take away from the statistical parameters below is that the maximum value for all of the continues parameters is about 1 and the mean average number is about 0.5. \n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data_description.png\" width=\"800\">\n",
    "<img src=\"data_description2.png\" width=\"600\">\n",
    "### <center>Figure 4: Data Description</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Now the skewness of the parameters are analyzed to get and understand of the overall distribution of the data. The only parameter that significantly stands out is \"loss\".\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"skew.png\" width=\"200\">\n",
    "### <center>Figure 5:  Skewness</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Figure 6 shows the loss data plotted on a histogram chart and we can see that the data is skewed to the right, so it needs to be normalized. Regression algorithms can be sensitive to the distribution of values and can results in the model underperforming if the data is not normally distributed. Figure 6 also shows logarithmic transformation applied to the \"loss\" data so that it does not negatively affect the performance of the learning algorithm. Appling the transformation will reduce the range of the values.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss data | Log Transformed Loss Data\n",
    "- | - \n",
    "<img src=\"loss_skew.png\" width=\"400\"> | <img src=\"loss_corr.png\" width=\"400\">\n",
    "### <center>Figure 6:  \"Loss\" Logorithmic Transformation</center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Shown in Figure 7 is the non-numerical data broken down into different components. The table below shows how many unique variables are in each column of non-numeric data.\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cat_data.png\" width=\"400\">\n",
    "### <center>Figure 7:  Number of Unique Non-Numerical Values</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Typically learning algorithms expect input to be numeric so all non-numeric data must be converted into numeric data. There are many ways to do so but for this project we will be using the pandas factorize operation. This method encodes the input values by assigning a unique number to each value.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"hotkey.png\" width=\"900\">\n",
    "### <center>Figure 8:  Non-nemeric Data Coverted to Numeric Data</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The data has been clearly defined and pre-processed. In Figure 9 is the end result that we need in order to start training out learning algorithm. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"contcat.png\" width=\"1000\">\n",
    "### <center>Figure 9:  Training Data After Cleaning</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cont1.png\" width=\"300\">\n",
    "<img src=\"cont2.png\" width=\"300\">\n",
    "<img src=\"cont3.png\" width=\"300\">\n",
    "<img src=\"cont4.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Before the data can be feed into the any learning algorithm it must be separated into a training and testing sets. Splitting the data and randomizing it removes any hidden bias or variances. This is because the model needs to be tested on unseen data. The mean absolute error of the testing model is then compared to the training model to check for over fitting or under fitting.\n",
    "\n",
    "The data is slit into a training and testing data sets.\n",
    "* Training set has 141238 samples.\n",
    "* Testing set has 47080 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three different repressors will be taken into consideration linear regression, random forest regression, and extreme gradient regression. The mean absolute error of a un-tuned Linear regression model will be used as our bench mark that we need to beat.\n",
    "\n",
    "#### Linear Regression\n",
    "* Linear Regression is used to model the relationship between two variables\n",
    "* It does show by plotting a line and minimizing the distance between that line and the data as show in Figure 10\n",
    "\n",
    "<img src=\"linear.png\" width=\"200\">\n",
    "### <center>Figure 10:  Linear Regression Example</center>\n",
    "\n",
    "#### XGBoost (Extreame Gradient Decent)\n",
    "* XGboosting stands for extreme gradient boosting algorithm and is part of a group of learning algorithms called ensemble methods. There are three types of ensemble methods and XGBoosting is a part of the boosting class of ensemble methods. XGBoosting runs multiple decision tree algorithms and each of which learns to fix the prediction errors of a prior model in the chain. In other words, this can be compared to how a basketball player shoots a basketball. If the basketball player misses the shot then the next shot the basketball player takes is corrected based on how far off his first shot was from the goal. The basketball player will continue to adjust this technique until he makes perfect shots.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "\n",
    "The data in training on an un-tuned Linear Regression and XGBoost Model to set our baseline. The benchmark for this project is set by the un-tuned Linear Regression model as a basis for determining how well our XGBoost Model performs. Mean Absolute Error is the metric that will be used for determining how good our models perform. The lower the mean absolute error is the better our model is performing. Figure 11 show the lowest mean absolute error for using Linear Regression is 1334.36 and that will be the benchmark for the XGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"linearxgb.png\" width=\"400\">\n",
    "### <center>Figure 11:  Linear Regression and XGBoost Results</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation and Refinement\n",
    "\n",
    "Initially the XGBoost algorithm was implement that was in the form of an numpy array. Early on training and predicting the models was very time consuming taking up to and hour to train what I thought was simple model. This made me realize that I was more than likely doing something wrong. I went back through the XGBoost documentation to make sure that I understood exactly how the algorithm worked. Through this I found out that XGBoost does not work efficiently with numpy arrays and instead prefers to use a Dmatrix. The Dmatrix is a data structure that is specifically used by XGBoost in order to optimize memory and training speed.\n",
    "\n",
    "Running the XGBoost algorithm using a linear objective function and with 50 boosting rounds results in a mean absolute error of 1178.91. \n",
    "The following parameters were tuned in order to optimize the model:\n",
    "* eta - learning rate of the model\n",
    "* max_depth - This is the maximum depth of the tree. A low number will probably not capture nonlinear features and a high number makes the model more complex.\n",
    "* min_child_weight - This parameter prevents under fitting\n",
    "* gamma - This parameters prevents overfitting\n",
    "* colsample_bytree -  a subsample ratio of columns when constructing each tree \n",
    "* subsample - defines a subsample ratio of training examples per tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Validation\n",
    "\n",
    "After fine tuning the model using various parameters the best answer was achieved using the following parameters.\n",
    "***\n",
    "xgb_params = {'eta': 0.04, \n",
    "    'colsample_bytree': 0.6, \n",
    "    'subsample': 1, \n",
    "    'objective': 'reg:linear',\n",
    "    'max_depth': 14,\n",
    "    'min_child_weight': 16,\n",
    "    'gamma': 0.6\n",
    "}\n",
    "***\n",
    "Using these parameters, a Mean Absolute Error of 1153 was found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification\n",
    "Since this is a Kaggle competition it is difficult to say whether the problem has been solved or not. The only thing we can go by is by the benchmark that was set and by comparing with other Kaggle scores. The final model outperformed the benchmark score of 1334. Currently the top Kaggle score is 1096 which is 189 points lower than what was calculated here. The model still needs work but for the purpose of this project it has met all expectation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "* In this project a data set was taken and processed in order to make it usable for the learning algorithm. Two different types of learning algorithms were used in this project, linear regression and XGBoost. An un-tuned linear regression model was used as a base model to compare our XGBoost model to. An un-tuned XGBoost model had a mean absolute error of 1178 but by tuning various parameters the mean absolute error was brought down to 1153.\n",
    "* This model was very complex in the sense that it had many parameters and allot of data points. This posed many challenges and limitations on obtaining a better solution. The fact that the data set was so large made running models very time consuming. \n",
    "* This was a very interesting and eye-opening project and motivates me to learn more about data science and machine learning. I have a couple of friends that work for large corporations like Walmart and AT&T in the automation and data science departments. Their job is find area of work were automation can be implemented in order reduce cost. In some cases, it can turn into a tense situation because people are losing their jobs, but this is the way forward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement\n",
    "\n",
    "There is always more work to be done but running these models is very time consuming. Another option might be to use cloud computing to speed up the process but that is out of the scope of this project. One option that I did not consider until now is finding the features that are most important and focusing more on those. This might help eliminate noise cause by features with little relevance. I used XGBoost because it was simple to understand but maybe in the future I will try to solve this problem using a Convolution Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
