{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Allstate Claim Severity Capstone Project\n",
    "Jose Garcia  \n",
    "December 31st, 2050\n",
    "\n",
    "## I. Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Overview\n",
    "\n",
    "Allstate is on the largest personal insurance companies in the United states and protects millions of people from potentials damages. When car accidents happen or when other unexpected disasters occur it is a huge relief knowing you are cover by some type of insurance. Recently my girlfriend was involved in a car accident that almost totaled her car. Although she escaped with no bodily injuries her car had thousands of dollars in damages. Luckily, she has full coverage insurance and was able to get all repairs paid for and a rental car without paying anything out of pocket. This event peaked my interest in to insurance business which lead me to this project. Although for this particular instance the insurance claim process was easy and straight forward it is not always the case. This is why Allstate is currently developing automated methods of predicting the cost, and hence severity, of a claim. This project will provide insight into better ways to predict claims severity.\n",
    "\n",
    "Dataset can be found on the Kaggle website at https://www.kaggle.com/c/allstate-claims-severity/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets and Inputs\n",
    "\n",
    "For this project Allstate provides a test.csv and train.csv. \n",
    "\n",
    "The train.csv contains the following:\n",
    " * claim id\n",
    " * cat 1 to cat 116 - this data is category based and contains either single letters or two letters\n",
    " * cont1 to cont14 - this data is a continues set of numbers that are not negative\n",
    " * loss - this is the ammount that the insurance company has to payout and this is also the target variable\n",
    " * 188,318 rows\n",
    " * 132 columns\n",
    " \n",
    "The test.csv file containt the same information as train.csv except for the loss because it needs to be predicted.\n",
    " * 89,023 rows\n",
    " * 131 columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "Allstate is attempting to improve its customer experience by automating methods of predicting the severity of a claim. The severity of a claim is based on a combination of different parameters that are unique to every claim. An insurance adjuster takes a look at each claim and all of its parameters and determines how much will be paid out to the customer. In some cases, the claim is quickly resolved but there is some instances in which it takes a long time to resolve the claim. For this project a training set of data will be used to train a machine learning algorithm to predict the cost of a claim based on the given parameters of the claim. This data will be used to train a linear regression learning algorithm to predict the loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "\n",
    "Kaggle will be evaluating this project using mean absolute error and the lower the mean absolute error the better. Of course, there are many other ways to evaluate this model but for the Kaggle competition this is the way the project entries are evaluated. Mean absolute error is a measure of the average difference between the actual loss and predicted loss. For this particular data set mean absolute error is a good measure of how far off we are from the actual value because it weighs each error the same. As opposed to root mean square error which adds more weight to the error as the difference between the predicted an actual value gets larger. \n",
    "\n",
    "<img src=\"mean-absolute-error.png\" width=\"300\">\n",
    "### <center>Figure 1: Mean Aboslute Error</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the first five rows of the \"trian.csv\" data set. The data set is composed of an id number, category data, continues data, and a column labeled \"loss\". There is a total of 188,318 rows and 132 columns of data but not all of these parameters are useful for building our predictive model. The column labeled \"id\" will be dropped because it does not add any value to our algorithm. The \"loss\" column will also be dropped because this is the value that needs to be predicted and should not be mixed in with our features. Now there are only two types of data, non-numeric, which is composed of letters and numerical data which is composed of continues numbers. There is a total of 14 columns of continues data and 116 columns of non-numeric data. Right below the \"train.csv\" data set is the \"test.csv\" data which is exactly the same except that it does not have the \"loss\" column. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"train_data.png\" width=\"1000\">\n",
    "### <center>Figure 2: train.csv</center>\n",
    "\n",
    "<img src=\"test_data.png\" width=\"1000\">\n",
    "### <center>Figure 3: test.csv</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "From this point on the \"train.csv\" data is the only data set that will be analyzed.\n",
    "\n",
    "In order to get a better understanding of the data it must be represented in different forms in order to get different types of information. Running basic statistical analysis on the continues data reveals key facts. A few things to take away from the statistical parameters below is that the maximum value for all of the continues parameters is about 1 and the mean average number is about 0.5. Because the range of the continues numbers is small they are prefect to use with a learning algorithm. The \"loss\" column on the other hand has a wide range of values and this will be problematic when applying it to a learning algorithm.\n",
    "\n",
    "\n",
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"data_description.png\" width=\"800\">\n",
    "<img src=\"data_description2.png\" width=\"600\">\n",
    "### <center>Figure 4: Data Description</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Now the skewness of the parameters are analyzed to get and understand of the overall distribution of the data. The only parameter that significantly stands out is \"loss\".\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"skew.png\" width=\"200\">\n",
    "### <center>Figure 5:  Skewness</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Figure 6 shows the loss data plotted on a histogram chart and we can see that the data is skewed to the right, so it needs to be normalized. Regression algorithms can be sensitive to the distribution of values and can results in the model underperforming if the data is not normally distributed. Figure 6 also shows logarithmic transformation applied to the \"loss\" data so that it does not negatively affect the performance of the learning algorithm. Appling the transformation will reduce the range of the values.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss data | Log Transformed Loss Data\n",
    "- | - \n",
    "<img src=\"loss_skew.png\" width=\"400\"> | <img src=\"loss_corr.png\" width=\"400\">\n",
    "### <center>Figure 6:  \"Loss\" Logorithmic Transformation</center>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Shown in Figure 7 is the non-numerical data broken down into different components. The table below shows how many unique variables are in each column of non-numeric data.\n",
    "\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cat_data.png\" width=\"400\">\n",
    "### <center>Figure 7:  Number of Unique Non-Numerical Values</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Typically learning algorithms expect input to be numeric so all non-numeric data must be converted into numeric data. There are many ways to do so but for this project we will be using the pandas factorize operation. This method encodes the input values by assigning a unique number to each value.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"hotkey.png\" width=\"900\">\n",
    "### <center>Figure 8:  Non-nemeric Data Coverted to Numeric Data</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "The data has been clearly defined and pre-processed. In Figure 9 is the end result that we need in order to start training out learning algorithm. \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"contcat.png\" width=\"1000\">\n",
    "### <center>Figure 9:  Training Data After Cleaning</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"cont1.png\" width=\"300\">\n",
    "<img src=\"cont2.png\" width=\"300\">\n",
    "<img src=\"cont3.png\" width=\"300\">\n",
    "<img src=\"cont4.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Before the data can be feed into the any learning algorithm it must be separated into a training and testing sets. Splitting the data and randomizing it removes any hidden bias or variances. This is because the model needs to be tested on unseen data. The mean absolute error of the testing model is then compared to the training model to check for over fitting or under fitting.\n",
    "\n",
    "The data is slit into a training and testing data sets.\n",
    "* Training set has 141238 samples.\n",
    "* Testing set has 47080 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms and Techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three different repressors will be taken into consideration linear regression, random forest regression, and extreme gradient regression. The mean absolute error of a un-tuned Linear regression model will be used as our bench mark that we need to beat.\n",
    "\n",
    "#### Linear Regression\n",
    "* Linear Regression is used to model the relationship between two variables\n",
    "* It does this by plotting a line and minimizing the distance between that line and the data as show in Figure 10\n",
    "* It great starting point when analyzing data because of its simplicity\n",
    "* The error increases when trying to apply this to non linear data\n",
    "\n",
    "\n",
    "<img src=\"linear.png\" width=\"200\">\n",
    "### <center>Figure 10:  Linear Regression Example</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost (Extreame Gradient Decent)\n",
    "* XGboosting stands for extreme gradient boosting algorithm and is part of a group of learning algorithms called ensemble methods. \n",
    " * There are three types of ensemble methods and XGBoosting is a part of the boosting class of ensemble methods.\n",
    " * XGBoosting runs multiple decision tree algorithms and each of which learns to fix the prediction errors of a prior model in the chain. \n",
    " * This can be compared to how a basketball player shoots a basketball. If the basketball player misses the shot, then the next shot the basketball player takes is corrected based on how far off his first shot was from the goal. The basketball player will continue to adjust this technique until he makes perfect shots. \n",
    " * Below in **Figure 11** is the Newton Tree Boosting algorithm used by XGBoosting\n",
    " * The main advantage of XGBoosting is the executiong speed and accuracy\n",
    " * A disadvantage is that it is prone to over fitting\n",
    "\n",
    "\n",
    "<img src=\"newton.png\" width=\"500\">\n",
    "### <center>Figure 11:  Newton Tree Boosting</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "\n",
    "The data in training on an un-tuned Linear Regression and XGBoost Model to set our baseline. The benchmark for this project is set by the un-tuned Linear Regression model as a basis for determining how well our XGBoost Model performs. Mean Absolute Error is the metric that will be used for determining how good our models perform. The lower the mean absolute error is the better our model is performing. The train.csv file has a total of 188,318 data points and this is split up in to 25% training and 74% testing. In **Figure 11** below an un-tuned linear regression and XGB regression algorithm area applied to 1%, 10%, an 100% of the data to see how the error changes as you add more data. The lowest mean absolute error for using Linear Regression is 1341.72 and that will be the benchmark for the XGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"linearxgb.png\" width=\"400\">\n",
    "### <center>Figure 12:  Linear Regression and XGBoost Results</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation and Refinement\n",
    "\n",
    "Initially the XGBoost algorithm was implement that was in the form of an numpy array. Early on training and predicting the models was very time consuming taking up to and hour to train what I thought was simple model. This made me realize that I was more than likely doing something wrong. I went back through the XGBoost documentation to make sure that I understood exactly how the algorithm worked. Through this I found out that XGBoost does not work efficiently with numpy arrays and instead prefers to use a Dmatrix. The Dmatrix is a data structure that is specifically used by XGBoost in order to optimize memory and training speed.\n",
    "\n",
    "Running the XGBoost algorithm using a linear objective function and with 50 boosting rounds results in a mean absolute error of 1162. \n",
    "The following parameters were tuned in order to optimize the model:\n",
    "* eta - learning rate of the model\n",
    "* max_depth - This is the maximum depth of the tree. A low number will probably not capture nonlinear features and a high number makes the model more complex.\n",
    "* min_child_weight - This parameter prevents under fitting\n",
    "\n",
    "The data exploration section shows and explains how the data was preprocessed. Now the next step to separate the data in to training and testing sets. After which we can begin to train the algorithm but before anything can be done the correct libraries must be imported.\n",
    "\n",
    "They are:\n",
    " * from sklearn.cross_validation import train_test_split - this will allow the data to be split into tranining and testing sets\n",
    " * from sklearn.metrics import mean_absolute_error - this will calculate mean absolute error\n",
    " * from sklearn.metrics import make_scorer - allows means absolute error to be used with XGboost\n",
    " * from time import time - this will import a clock to keep track of time\n",
    " * import xgboost as xgb - this will import extreme gradient boosting and all functions associated with it\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The following steps were taken to tune the algorithm.\n",
    "\n",
    "For each of the follow sets of parameters a for loop was created to train each combination of parameters. After each set an optimal parameter was found and added to the next set of parameters.\n",
    "\n",
    "1.  'eta': [ 0.2, 0.3, 0.4, 0.5, 0.6] was trained with num_boost_round = 50\n",
    "    * optimal param = 'eta': 0.3\n",
    "    * MAE = 1162\n",
    " \n",
    "2. 'eta': 0.3, 'max_depth': [6,8,10,12,14] , 'child_weight': [4,6,8,10,12], 'num_boost_round': 50\n",
    "    * optimal param = eta': 0.3, 'max_depth': 6 , 'child_weight': 8, \n",
    "    'num_boost_round': 50\n",
    "    * MAE = 1160\n",
    "    \n",
    "3. 'eta': [0.05, 0.1, 0.2, 0.3], 'max_depth': 6 , 'child_weight': 8, 'num_boost_round': 100\n",
    "    * optimal param = eta': 0.2, 'max_depth': 6 , 'child_weight': 8, \n",
    "    'num_boost_round': 100\n",
    "    * MAE = 1152\n",
    "\n",
    "4. 'eta': [0.05, 0.1, 0.2, 0.3], 'max_depth': 6 , 'child_weight': 8, 'num_boost_round': 200\n",
    "    * optimal param = eta': 0.1, 'max_depth': 6 , 'child_weight': 8, \n",
    "    'num_boost_round': 200\n",
    "    * MAE = 1147\n",
    "    \n",
    "Below is a basic example of how the learning algorithm was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This dictionary contains all parameters that will be used\n",
    "xgb_params = {'objective': 'reg:linear'}\n",
    "\n",
    "# The scoring function is assigned\n",
    "scorer = make_scorer(mean_absolute_error)\n",
    "\n",
    "\n",
    "results = {}\n",
    "start = time()\n",
    "\n",
    "# The algorithm is trained with the given parameters\n",
    "bst_cv1 = xgb.train(params=xgb_params, dtrain=dtrain, num_boost_round=50,feval=scorer)\n",
    "end = time()\n",
    "results['train_time'] = end - start\n",
    "\n",
    "start = time()\n",
    "# predictions are made on the training and testing data\n",
    "predictions_train = bst_cv1.predict(dtrain)\n",
    "predictions_test = bst_cv1.predict(dtest)\n",
    "end = time()\n",
    "results['pred_time'] = end - start\n",
    "\n",
    "# The predictions are pulled from the Dmatrix\n",
    "y_tr = dtrain.get_label()\n",
    "y_te = dtest.get_label()\n",
    "\n",
    "\n",
    "# The results are first normalized because they are in exponential form.\n",
    "# Then the mean absolute error can be calculated\n",
    "results['acc_train'] = mean_absolute_error(np.exp(y_tr), np.exp(predictions_train))\n",
    "results['acc_test'] = mean_absolute_error(np.exp(y_te), np.exp(predictions_test))\n",
    "\n",
    "# The code below prints on the mean absolute error and time it took to train and prediction the model\n",
    "print \"params {}\".format(xgb_params)\n",
    "print \"Trained score = {:.2f} and test score = {:.2f}\".format(results['acc_train'],results['acc_test'])\n",
    "print \"Trained time = {:.2f} and pred time = {:.2f}\".format(results['train_time'],results['pred_time'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation and Validation\n",
    "\n",
    "After fine tuning the model using various parameters the best answer was achieved using the following parameters.\n",
    "***\n",
    "xgb_params = {'eta': 0.1, 'objective': 'reg:linear', 'max_depth': 6, 'min_child_weight': 8,}\n",
    "***\n",
    "Using these parameters, a Mean Absolute Error of 1147 was found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification\n",
    "Since this is a Kaggle competition it is difficult to say whether the problem has been solved or not. The only thing we can go by is by the benchmark that was set and by comparing with other Kaggle scores. The final model outperformed the benchmark score of 1334. Currently the top Kaggle score is 1096 which is 189 points lower than what was calculated here. The model still needs work but for the purpose of this project it has met all expectation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "The most important part of this project was the visualization of the \"loss\" parameter. It is not clear what the data is doing until you plot it as a histogram. The picture left of **Figure 13** is the original data and its import to note that it is skewed to the right. If this data were to be used as is then the learning algorithm would be very inefficient because of the skewed distribution and large range of values. The data on the right is the \"loss\" data after it has been transformed using the log function. This set of data has an even distribution and work well with learning algorithm. One thing to keep in mind is that after the learning algorithm has been applied the \"loss\" data has to been transformed back using the exponent function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss data | Log Transformed Loss Data\n",
    "- | - \n",
    "<img src=\"loss_skew.png\" width=\"400\"> | <img src=\"loss_corr.png\" width=\"400\">\n",
    "### <center>Figure 13:  \"Loss\" Logorithmic Transformation</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "* In this project a data set was taken and processed in order to make it usable for the learning algorithm. Two different types of learning algorithms were used in this project, linear regression and XGBoost. An un-tuned linear regression model was used as a base model to compare our XGBoost model to. An un-tuned XGBoost model had a mean absolute error of 1178 but by tuning various parameters the mean absolute error was brought down to 1153.\n",
    "* This model was very complex in the sense that it had many parameters and allot of data points. This posed many challenges and limitations on obtaining a better solution. The fact that the data set was so large made running models very time consuming. \n",
    "* This was a very interesting and eye-opening project and motivates me to learn more about data science and machine learning. I have a couple of friends that work for large corporations like Walmart and AT&T in the automation and data science departments. Their job is find area of work were automation can be implemented in order reduce cost. In some cases, it can turn into a tense situation because people are losing their jobs, but this is the way forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improvement\n",
    "\n",
    "There is always more work to be done but running these models is very time consuming. Another option might be to use cloud computing to speed up the process but that is out of the scope of this project. One option that I did not consider until now is finding the features that are most important and focusing more on those. This might help eliminate noise cause by features with little relevance. I used XGBoost because it was simple to understand but maybe in the future I will try to solve this problem using a Convolution Neural Network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### References\n",
    "\n",
    " 1. Allstate Claims Severity, Kaggle \n",
    "    * https://www.kaggle.com/c/allstate-claims-severity\n",
    "    * https://www.kaggle.com/c/allstate-claims-severity/data\n",
    "      \n",
    " 2. Regression\n",
    "    * https://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/\n",
    "    * https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/\n",
    "    \n",
    " 3. Allstate Insurance\n",
    "    * https://en.wikipedia.org/wiki/Allstate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
