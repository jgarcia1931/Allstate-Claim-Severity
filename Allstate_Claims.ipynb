{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Allstate Insurance Capstone Proposal\n",
    "Jose Manuel Garcia  \n",
    "February 18, 2018\n",
    "\n",
    "Allstate Claim Severity Predictive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Domain Background\n",
    "\n",
    "Allstate is on the largest personal insurance companies in the United states and protect millions of people from potentials damages. When car accidents happen or when other unexpected disasters occur it is a huge relief knowing you are cover by some type of insurance. Recent my girlfriend was involved in a car accident that almost totaled her car. Although she escaped with no bodily injuries her car had thousands of dollars in damages. Luckily, she has full coverage insurance and was able to get all repairs paid for and a rental car without paying anything out of pocket. This event peaked my interest in the insurance business which lead me to this project. Although for this particular instance the insurance claim process was easy and straight forward it is not always the case. This is why Allstate is currently developing automated methods of predicting the cost, and hence severity, of claims. This project will provide insight into better ways to predict claims severity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement\n",
    "\n",
    "Allstate is attempting to improve its customer experience by automating methods of predicting the severity of the claim. The severity of a claim is based on my different parameters that are unique to every claim. An insurance adjuster takes a look at each claim and all of its parameters and determines how much will be paid out. In some cases the claim is quickly resolved but there is some instances in which it takes a long time to resolve the claim. For this project a train set of data will be used to train a machine learning algorithm to be able to predict the cost of a claim based on the given parameters of the claim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets and Inputs\n",
    "\n",
    "For this project Allstate provides a test.csv and train.csv. \n",
    "\n",
    "The train.csv contains the following:\n",
    " * claim id\n",
    " * cat 1 to cat 116 - this data is category based and contains either single letters or two letters\n",
    " * cont1 to cont14 - this data is a continues set of numbers that are not negative\n",
    " * loss - this is the ammount that the insurance company has to payout and this is also the target variable\n",
    " * 188,318 rows\n",
    " * 132 columns\n",
    " \n",
    "The test.csv file containt the same information as train.csv except for the loss because it needs to be predicted.\n",
    " * 89,023 rows\n",
    " * 131 columns\n",
    "\n",
    "https://www.kaggle.com/c/allstate-claims-severity/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Statement\n",
    "\n",
    "The objective is to find the relationship between the 130 features and the value of loss. The first step that needs to be taken is to clean and restructure all of the data. Doing so will help improve the predictive power of the learning algorithm. This is because most algorithms can be sensitive to the distribution of values in the data and canâ€™t lead to poor performance if the data is not properly normalized. Normalizing also ensures that each feature is treated equally when applying supervised learners. In this data set, there is also features that are represented by letters. Typically learning algorithms require inputs to be numbers and cannot be training using letters. A solution to this is using one-hot encoding scheme that creates dummy variables for each possible category of non-numeric variables. \n",
    "\n",
    "This is a regression problem so Linear Regression, RandomForestRegressor, and XGBoost Regressor will be used. Samples of the data will be taken to train a model using each regressor and the one with the lowest mean absolute error will be used for the rest of the project. The models parameters will be tuned using GridSearchCV and the mean squared error will be used as a scorer. Then the features that provide the most predictive power to the model will be determined. Reducing the number of features to the most important ones reduces the complexity of the model which is what we want. This will be done by using the feature importance_ attribute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark Model\n",
    "\n",
    "Since this a Kaggle competition hosted by Allstate Insurance the benchmark is set by the best Kaggle score on the leadership boards which is currently 1109.70772 mean absolute error. The goal is to get as close as possible score but as of now I am hopping to score in the top 25%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "Kaggle will be evaluating this project using mean absolute error and the lower the mean absolute error the better. Of course, there are many other ways to evaluate this model but for the Kaggle competition this is the way the project entries are evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Design\n",
    "\n",
    "A big part of machine learning is acquiring the data to build your model and making sure it is clean and reliable. For this project Allstate has already done most of this work by providing all of the data for this Kaggle competition. The next step will be to process the data by scaling, normalizing and converting non-numeric data into numbers. This will greatly improve the predictive power of the model. Once we have processed the data set a sample of the data will be trained using Linear Regression, RandomForestRegressor, and XGBoost Regressor. The performance will be evaluated using mean absolute error and the model with the lowest mean absolute error will be used for the rest of the project. The data will then be shuffled and slip in to training and testing data. The model and will then be tuned using GridSearchCV and the scorer that will be used is mean absolute error. Next the most relevant features will be found using the feature importance attribute. Once the most relevant features are known they will be trained using the model found using GridSearchCV its performance will be compared to the model that used all of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### References\n",
    " 1. Allstate Claims Severity, Kaggle \n",
    "    * https://www.kaggle.com/c/allstate-claims-severity\n",
    "      \n",
    " 2. Regression\n",
    "    * https://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/\n",
    "    * https://machinelearningmastery.com/gentle-introduction-xgboost-applied-machine-learning/\n",
    "    \n",
    " 3. Allstate Insurance\n",
    "    * https://en.wikipedia.org/wiki/Allstate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
